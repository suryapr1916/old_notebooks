{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of practice_vi.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srF3gxByPAb6"
      },
      "source": [
        "### Markov decision process\n",
        "\n",
        "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
        "\n",
        "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
        "\n",
        "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/blob/36a0b58261acde756abd55306fbe63df226bf62b/hw2/HW2.ipynb) _by Berkeley_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kjBmicDPAb7"
      },
      "source": [
        "For starters, let's define a simple MDP from this picture:\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" alt=\"Diagram by Waldoalvarez via Wikimedia Commons, CC BY-SA 4.0\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4npySV8nPAb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a2b71bb-384a-4d95-9c5e-c514ffcb1d74"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week2_model_based/submit.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week2_model_based/mdp.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBnwPJ0VPAb_"
      },
      "source": [
        "transition_probs = {\n",
        "    's0': {\n",
        "        'a0': {'s0': 0.5, 's2': 0.5},\n",
        "        'a1': {'s2': 1}\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "        'a1': {'s1': 0.95, 's2': 0.05}\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': {'s0': 0.4, 's2': 0.6},\n",
        "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "rewards = {\n",
        "    's1': {'a0': {'s0': +5}},\n",
        "    's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "\n",
        "from mdp import MDP\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt7wNydSPAcC"
      },
      "source": [
        "We can now use MDP just as any other gym environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2VBprRGPAcC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "dce449f5-a606-4366-95ce-523e4737f555"
      },
      "source": [
        "print('initial state =', mdp.reset())\n",
        "next_state, reward, done, info = mdp.step('a1')\n",
        "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial state = s0\n",
            "next_state = s2, reward = 0.0, done = False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eyjUcjNPAcG"
      },
      "source": [
        "but it also has other methods that you'll need for Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UHNloMJPAcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "23ab97e1-c421-4cc0-ac66-f84fdd1fd2ab"
      },
      "source": [
        "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
        "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
        "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
        "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
        "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mdp.get_all_states = ('s0', 's1', 's2')\n",
            "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
            "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
            "mdp.get_reward('s1', 'a0', 's0') =  5\n",
            "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH6AswhgPAcK"
      },
      "source": [
        "### Optional: Visualizing MDPs\n",
        "\n",
        "You can also visualize any MDP with the drawing fuction donated by [neer201](https://github.com/neer201).\n",
        "\n",
        "You have to install graphviz for system and for python. For ubuntu just run:\n",
        "\n",
        "1. `sudo apt-get install graphviz`\n",
        "2. `pip install graphviz`\n",
        "3. restart the notebook\n",
        "\n",
        "__Note:__ Installing graphviz on some OS (esp. Windows) may be tricky. However, you can ignore this part alltogether and use the standart vizualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aGMvRe1PAcK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f8df95f-5545-4176-9a19-e8c7c2f65890"
      },
      "source": [
        "from mdp import has_graphviz\n",
        "from IPython.display import display\n",
        "print(\"Graphviz available:\", has_graphviz)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graphviz available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBF402VdPAcN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "31f59bde-97c7-4be5-deef-b59f555ca986"
      },
      "source": [
        "if has_graphviz:\n",
        "    from mdp import plot_graph, plot_graph_with_state_values, plot_graph_optimal_strategy_and_state_values\n",
        "    display(plot_graph(mdp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fae078028d0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: MDP Pages: 1 -->\n<svg width=\"720pt\" height=\"226pt\"\n viewBox=\"0.00 0.00 720.00 225.64\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.7008 .7008) rotate(0) translate(4 318)\">\n<title>MDP</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-318 1023.463,-318 1023.463,4 -4,4\"/>\n<!-- s0 -->\n<g id=\"node1\" class=\"node\">\n<title>s0</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"40\" cy=\"-116\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"40\" cy=\"-116\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-109.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">s0</text>\n</g>\n<!-- s0&#45;a0 -->\n<g id=\"node2\" class=\"node\">\n<title>s0&#45;a0</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"193.5772\" cy=\"-160\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"193.5772\" y=\"-155\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a0</text>\n</g>\n<!-- s0&#45;&gt;s0&#45;a0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>s0&#45;&gt;s0&#45;a0</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M79.2844,-124.7026C99.8974,-129.5498 125.4811,-136.0095 148,-143 151.327,-144.0328 154.7652,-145.1741 158.1924,-146.3622\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"157.1257,-149.698 167.7202,-149.7831 159.4912,-143.1097 157.1257,-149.698\"/>\n</g>\n<!-- s0&#45;a1 -->\n<g id=\"node4\" class=\"node\">\n<title>s0&#45;a1</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"193.5772\" cy=\"-233\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"193.5772\" y=\"-228\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a1</text>\n</g>\n<!-- s0&#45;&gt;s0&#45;a1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>s0&#45;&gt;s0&#45;a1</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M66.6878,-145.9984C76.104,-155.7862 87.0765,-166.3514 98,-175 117.3546,-190.324 141.0693,-204.7377 160.0344,-215.3632\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"158.4757,-218.5005 168.923,-220.2624 161.8547,-212.37 158.4757,-218.5005\"/>\n</g>\n<!-- s0&#45;a0&#45;&gt;s0 -->\n<g id=\"edge2\" class=\"edge\">\n<title>s0&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M166.0433,-155.6398C146.7591,-152.2473 120.5165,-146.9481 98,-140 94.1846,-138.8227 90.2733,-137.4907 86.3715,-136.0724\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"87.3439,-132.6975 76.753,-132.4084 84.852,-139.239 87.3439,-132.6975\"/>\n<text text-anchor=\"middle\" x=\"123\" y=\"-158.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.5</text>\n</g>\n<!-- s2 -->\n<g id=\"node3\" class=\"node\">\n<title>s2</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"433.1543\" cy=\"-183\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"433.1543\" cy=\"-183\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"433.1543\" y=\"-176.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">s2</text>\n</g>\n<!-- s0&#45;a0&#45;&gt;s2 -->\n<g id=\"edge3\" class=\"edge\">\n<title>s0&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M221.0597,-162.6384C260.5326,-166.4279 333.9353,-173.4747 383.1716,-178.2015\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"383.0017,-181.7012 393.2904,-179.173 383.6707,-174.7333 383.0017,-181.7012\"/>\n<text text-anchor=\"middle\" x=\"307.1543\" y=\"-182.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.5</text>\n</g>\n<!-- s2&#45;a0 -->\n<g id=\"node8\" class=\"node\">\n<title>s2&#45;a0</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"666.7315\" cy=\"-162\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"666.7315\" y=\"-157\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a0</text>\n</g>\n<!-- s2&#45;&gt;s2&#45;a0 -->\n<g id=\"edge13\" class=\"edge\">\n<title>s2&#45;&gt;s2&#45;a0</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M473.2351,-186.9649C511.6217,-189.7355 570.9586,-191.3218 621.1543,-181 624.8123,-180.2478 628.5451,-179.2014 632.2213,-177.9828\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"633.5825,-181.2107 641.7302,-174.4383 631.1375,-174.6516 633.5825,-181.2107\"/>\n</g>\n<!-- s2&#45;a1 -->\n<g id=\"node9\" class=\"node\">\n<title>s2&#45;a1</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"666.7315\" cy=\"-80\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"666.7315\" y=\"-75\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a1</text>\n</g>\n<!-- s2&#45;&gt;s2&#45;a1 -->\n<g id=\"edge16\" class=\"edge\">\n<title>s2&#45;&gt;s2&#45;a1</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M467.3288,-161.7827C475.0423,-157.3286 483.2807,-152.8261 491.1543,-149 538.2751,-126.1024 595.0607,-104.8798 631.0354,-92.179\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"632.3234,-95.4365 640.6049,-88.8284 630.0101,-88.8297 632.3234,-95.4365\"/>\n</g>\n<!-- s0&#45;a1&#45;&gt;s2 -->\n<g id=\"edge5\" class=\"edge\">\n<title>s0&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M221.1044,-228.5867C256.8082,-222.6933 320.9391,-211.5305 375.1543,-199 378.3093,-198.2708 381.5506,-197.4836 384.8112,-196.6631\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"385.7835,-200.0268 394.5834,-194.1266 384.0248,-193.2513 385.7835,-200.0268\"/>\n<text text-anchor=\"middle\" x=\"307.1543\" y=\"-230.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 1</text>\n</g>\n<!-- s1 -->\n<g id=\"node5\" class=\"node\">\n<title>s1</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"829.3087\" cy=\"-116\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"829.3087\" cy=\"-116\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"829.3087\" y=\"-109.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">s1</text>\n</g>\n<!-- s1&#45;a0 -->\n<g id=\"node6\" class=\"node\">\n<title>s1&#45;a0</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"991.8858\" cy=\"-92\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"991.8858\" y=\"-87\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a0</text>\n</g>\n<!-- s1&#45;&gt;s1&#45;a0 -->\n<g id=\"edge6\" class=\"edge\">\n<title>s1&#45;&gt;s1&#45;a0</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M869.2486,-112.1898C891.9249,-109.7844 920.7924,-106.3154 946.3087,-102 949.1542,-101.5187 952.0904,-100.9778 955.0369,-100.403\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"955.889,-103.8011 964.9779,-98.3566 954.4775,-96.9449 955.889,-103.8011\"/>\n</g>\n<!-- s1&#45;a1 -->\n<g id=\"node7\" class=\"node\">\n<title>s1&#45;a1</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"991.8858\" cy=\"-174\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"991.8858\" y=\"-169\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a1</text>\n</g>\n<!-- s1&#45;&gt;s1&#45;a1 -->\n<g id=\"edge10\" class=\"edge\">\n<title>s1&#45;&gt;s1&#45;a1</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M867.494,-127.948C890.4784,-135.3328 920.2423,-145.2373 946.3087,-155 949.8059,-156.3098 953.4327,-157.7241 957.0437,-159.1692\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"955.8842,-162.4763 966.4653,-163.0165 958.5305,-155.9957 955.8842,-162.4763\"/>\n</g>\n<!-- s1&#45;a0&#45;&gt;s0 -->\n<g id=\"edge7\" class=\"edge\">\n<title>s1&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M972.3647,-72.0374C943.7154,-44.9794 887.3603,0 829.3087,0 193.5772,0 193.5772,0 193.5772,0 142.2271,0 97.1839,-41.8033 69.3807,-75.22\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"66.5993,-73.0932 63.0318,-83.0693 72.0419,-77.4953 66.5993,-73.0932\"/>\n<text text-anchor=\"middle\" x=\"556.1543\" y=\"-5.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.7 &#160;reward =5</text>\n</g>\n<!-- s1&#45;a0&#45;&gt;s2 -->\n<g id=\"edge9\" class=\"edge\">\n<title>s1&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M976.2683,-115.2719C972.0616,-122.1013 967.7329,-129.6999 964.3087,-137 953.7441,-159.5223 966.2691,-175.1526 946.3087,-190 783.9583,-310.7629 691.4414,-228.7477 491.1543,-200 487.8825,-199.5304 484.5378,-198.919 481.1875,-198.2077\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"481.7276,-194.7383 471.1878,-195.8166 480.0996,-201.5463 481.7276,-194.7383\"/>\n<text text-anchor=\"middle\" x=\"741.8087\" y=\"-258.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- s1&#45;a0&#45;&gt;s1 -->\n<g id=\"edge8\" class=\"edge\">\n<title>s1&#45;a0&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M965.6405,-83.4352C944.2075,-77.7535 913.319,-72.584 887.3087,-80 881.5406,-81.6446 875.7864,-84.0459 870.2659,-86.8404\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"868.2815,-83.9387 861.2349,-91.8506 871.6774,-90.0599 868.2815,-83.9387\"/>\n<text text-anchor=\"middle\" x=\"916.8087\" y=\"-85.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.1</text>\n</g>\n<!-- s1&#45;a1&#45;&gt;s2 -->\n<g id=\"edge12\" class=\"edge\">\n<title>s1&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M979.9034,-198.873C972.2373,-211.8113 960.9673,-226.464 946.3087,-234 766.4005,-326.4912 679.6137,-307.5164 491.1543,-234 482.8044,-230.7427 474.8606,-225.7094 467.6802,-220.0496\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"469.5052,-217.0066 459.623,-213.1866 464.9661,-222.3354 469.5052,-217.0066\"/>\n<text text-anchor=\"middle\" x=\"741.8087\" y=\"-301.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.05</text>\n</g>\n<!-- s1&#45;a1&#45;&gt;s1 -->\n<g id=\"edge11\" class=\"edge\">\n<title>s1&#45;a1&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M964.4796,-170.89C942.9286,-167.8208 912.4497,-162.0831 887.3087,-152 881.8827,-149.8239 876.3943,-147.1497 871.069,-144.2503\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"872.7141,-141.1589 862.3007,-139.2062 869.2235,-147.2265 872.7141,-141.1589\"/>\n<text text-anchor=\"middle\" x=\"916.8087\" y=\"-173.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.95</text>\n</g>\n<!-- s2&#45;a0&#45;&gt;s0 -->\n<g id=\"edge14\" class=\"edge\">\n<title>s2&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M640.4115,-153.6379C634.1509,-151.8874 627.4576,-150.2085 621.1543,-149 446.1882,-115.4541 399.1567,-130.324 221.1543,-123 177.0766,-121.1864 126.9601,-119.2603 90.6308,-117.8886\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"90.4191,-114.3783 80.2944,-117.4993 90.1556,-121.3733 90.4191,-114.3783\"/>\n<text text-anchor=\"middle\" x=\"307.1543\" y=\"-131.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.4</text>\n</g>\n<!-- s2&#45;a0&#45;&gt;s2 -->\n<g id=\"edge15\" class=\"edge\">\n<title>s2&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M639.3416,-157.9791C604.5225,-153.6753 542.7351,-148.7496 491.1543,-159 487.066,-159.8125 482.9045,-160.9179 478.7831,-162.2107\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"477.5839,-158.9221 469.2922,-165.5174 479.887,-165.5324 477.5839,-158.9221\"/>\n<text text-anchor=\"middle\" x=\"556.1543\" y=\"-164.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.6</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s0 -->\n<g id=\"edge17\" class=\"edge\">\n<title>s2&#45;a1&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M643.5482,-64.3784C636.6374,-60.5416 628.8477,-56.9714 621.1543,-55 456.6448,-12.8443 407.9727,-45.5381 239.1543,-64 175.7418,-70.9348 158.9077,-71.0397 98,-90 93.9106,-91.273 89.7281,-92.7576 85.5746,-94.3605\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"83.9293,-91.2516 75.9933,-98.271 86.5745,-97.7326 83.9293,-91.2516\"/>\n<text text-anchor=\"middle\" x=\"307.1543\" y=\"-69.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.3 &#160;reward =&#45;1</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s2 -->\n<g id=\"edge19\" class=\"edge\">\n<title>s2&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M640.4326,-71.0411C604.0521,-60.3658 537.6085,-47.2183 491.1543,-75 469.3384,-88.0469 455.2541,-112.4282 446.4357,-134.7277\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"443.1489,-133.5248 442.9716,-144.1181 449.7163,-135.9475 443.1489,-133.5248\"/>\n<text text-anchor=\"middle\" x=\"556.1543\" y=\"-80.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.4</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s1 -->\n<g id=\"edge18\" class=\"edge\">\n<title>s2&#45;a1&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M693.9433,-86.0256C717.3708,-91.2132 751.7657,-98.8294 780.0218,-105.0862\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"779.6023,-108.5781 790.1225,-107.3229 781.1157,-101.7436 779.6023,-108.5781\"/>\n<text text-anchor=\"middle\" x=\"741.8087\" y=\"-108.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.3</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN9wiZu1PAcQ"
      },
      "source": [
        "### Value Iteration\n",
        "\n",
        "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
        "\n",
        "Here's the pseudo-code for VI:\n",
        "\n",
        "---\n",
        "\n",
        "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
        "\n",
        "`2.` For $i=0, 1, 2, \\dots$\n",
        " \n",
        "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta1ZpTDWPAcQ"
      },
      "source": [
        "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
        "\n",
        "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EiXOXAJPAcR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87c04dbf-132a-4b6d-bb65-858022f6dedf"
      },
      "source": [
        "%%writefile mdp_get_action_value.py\n",
        "\n",
        "def get_action_value(mdp, state_values, state, action, gamma):\n",
        "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
        "    Q=0\n",
        "    for i in list(mdp.get_next_states(state, action).keys()):\n",
        "    Q += mdp.get_transition_prob(state, action, i)*(mdp.get_reward(state, action, i)+gamma*(state_values[i]))\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing mdp_get_action_value.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKj7yAeNPAcU"
      },
      "source": [
        "from mdp_get_action_value import get_action_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXEPD3IRPAcX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "84094767-ff74-4673-bd89-27f995c08519"
      },
      "source": [
        "import numpy as np\n",
        "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5a110522b853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_Vs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_action_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Vs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.69\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_action_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Vs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoNsakeRPAcZ"
      },
      "source": [
        "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
        " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHxZYbnTPAca"
      },
      "source": [
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return 0\n",
        "\n",
        "    <YOUR CODE>\n",
        "    \n",
        "    return <YOUR CODE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltmoJOP3PAcd"
      },
      "source": [
        "test_Vs_copy = dict(test_Vs)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
        "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
        "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
        "assert test_Vs == test_Vs_copy, \"Please do not change state_values in get_new_state_value\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TNMXkmgPAcf"
      },
      "source": [
        "Finally, let's combine everything we wrote into a working value iteration algo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swmDo6QGPAcg"
      },
      "source": [
        "# parameters\n",
        "gamma = 0.9            # discount for MDP\n",
        "num_iter = 100         # maximum iterations, excluding initialization\n",
        "# stop VI if new values are this close to old values (or closer)\n",
        "min_difference = 0.001\n",
        "\n",
        "# initialize V(s)\n",
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "if has_graphviz:\n",
        "    display(plot_graph_with_state_values(mdp, state_values))\n",
        "\n",
        "for i in range(num_iter):\n",
        "\n",
        "    # Compute new state values using the functions you defined above.\n",
        "    # It must be a dict {state : float V_new(state)}\n",
        "    new_state_values = <YOUR CODE>\n",
        "\n",
        "    assert isinstance(new_state_values, dict)\n",
        "\n",
        "    # Compute difference\n",
        "    diff = max(abs(new_state_values[s] - state_values[s])\n",
        "               for s in mdp.get_all_states())\n",
        "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
        "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
        "    state_values = new_state_values\n",
        "\n",
        "    if diff < min_difference:\n",
        "        print(\"Terminated\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-eRojoQPAck"
      },
      "source": [
        "if has_graphviz:\n",
        "    display(plot_graph_with_state_values(mdp, state_values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOOXM22PPAcm"
      },
      "source": [
        "print(\"Final state values:\", state_values)\n",
        "\n",
        "assert abs(state_values['s0'] - 3.781) < 0.01\n",
        "assert abs(state_values['s1'] - 7.294) < 0.01\n",
        "assert abs(state_values['s2'] - 4.202) < 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXpO0Of4PAco"
      },
      "source": [
        "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
        "\n",
        " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
        " \n",
        "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0dWQzOPAcp"
      },
      "source": [
        "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
        "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return None\n",
        "\n",
        "    <YOUR CODE>\n",
        "\n",
        "    return <YOUR CODE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evvAP0rLPAcr"
      },
      "source": [
        "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
        "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
        "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a1'\n",
        "\n",
        "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
        "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
        "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
        "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmNMPK7BPAct"
      },
      "source": [
        "if has_graphviz:\n",
        "    try:\n",
        "        display(plot_graph_optimal_strategy_and_state_values(mdp, state_values))\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Run the cell that starts with \\\"%%writefile mdp_get_action_value.py\\\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezLjyPnbPAcv"
      },
      "source": [
        "# Measure agent's average reward\n",
        "\n",
        "s = mdp.reset()\n",
        "rewards = []\n",
        "for _ in range(10000):\n",
        "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "    rewards.append(r)\n",
        "\n",
        "print(\"average reward: \", np.mean(rewards))\n",
        "\n",
        "assert(0.40 < np.mean(rewards) < 0.55)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtcmvTfFPAcx"
      },
      "source": [
        "### Frozen lake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCY1Mac5PAcy"
      },
      "source": [
        "from mdp import FrozenLakeEnv\n",
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "\n",
        "mdp.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvS5yfmsPAc0"
      },
      "source": [
        "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
        "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
        "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
        "    for i in range(num_iter):\n",
        "\n",
        "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
        "        new_state_values = <YOUR CODE>\n",
        "\n",
        "        assert isinstance(new_state_values, dict)\n",
        "\n",
        "        # Compute difference\n",
        "        diff = max(abs(new_state_values[s] - state_values[s])\n",
        "                   for s in mdp.get_all_states())\n",
        "\n",
        "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" %\n",
        "              (i, diff, new_state_values[mdp._initial_state]))\n",
        "\n",
        "        state_values = new_state_values\n",
        "        if diff < min_difference:\n",
        "            break\n",
        "\n",
        "    return state_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH6pD62sPAc2"
      },
      "source": [
        "state_values = value_iteration(mdp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCiRh8UTPAc4"
      },
      "source": [
        "s = mdp.reset()\n",
        "mdp.render()\n",
        "for t in range(100):\n",
        "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
        "    print(a, end='\\n\\n')\n",
        "    s, r, done, _ = mdp.step(a)\n",
        "    mdp.render()\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBnQay6-PAc7"
      },
      "source": [
        "### Let's visualize!\n",
        "\n",
        "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6g0ao8NPAc7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def draw_policy(mdp, state_values):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    h, w = mdp.desc.shape\n",
        "    states = sorted(mdp.get_all_states())\n",
        "    V = np.array([state_values[s] for s in states])\n",
        "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
        "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(h)-.5)\n",
        "    ax.set_yticks(np.arange(w)-.5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    Y, X = np.mgrid[0:4, 0:4]\n",
        "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (0, 1)}\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
        "                     color='g', size=12,  verticalalignment='center',\n",
        "                     horizontalalignment='center', fontweight='bold')\n",
        "            a = Pi[y, x]\n",
        "            if a is None:\n",
        "                continue\n",
        "            u, v = a2uv[a]\n",
        "            plt.arrow(x, y, u*.3, -v*.3, color='m',\n",
        "                      head_width=0.1, head_length=0.1)\n",
        "    plt.grid(color='b', lw=2, ls='-')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvlFOqdhPAc-"
      },
      "source": [
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"after iteration %i\" % i)\n",
        "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "# please ignore iter 0 at each step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUi979bMPAdA"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(30):\n",
        "    clear_output(True)\n",
        "    print(\"after iteration %i\" % i)\n",
        "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "    sleep(0.5)\n",
        "# please ignore iter 0 at each step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1yxmzJPAdC"
      },
      "source": [
        "Massive tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5MplNOQPAdD"
      },
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxJh8b8HPAdF"
      },
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w539Gv2CPAdH"
      },
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDyxnVTTPAdK"
      },
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LCZP8FsPAdM"
      },
      "source": [
        "### Submit to coursera\n",
        "\n",
        "If your submission doesn't finish in 30 seconds, set `verbose=True` and try again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usrIbTmuPAdM"
      },
      "source": [
        "from submit import submit_assigment\n",
        "submit_assigment(\n",
        "    get_action_value,\n",
        "    get_new_state_value,\n",
        "    get_optimal_action,\n",
        "    value_iteration,\n",
        "    'your.email@example.com',\n",
        "    'YourAssignmentToken',\n",
        "    verbose=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}